{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyek Pengembangan dan Pengoperasian Sistem Machine Learning**\n",
    "\n",
    "* Nama : Moh. Aflah Azzaky\n",
    "* Email : aflahazzaki123@gmail.com\n",
    "* ID Dicoding : aflahazzaky\n",
    "* Dataset : [Heart Failure Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Buat folder 'modules, config, monitoring' jika belum ada\n",
    "os.makedirs('modules', exist_ok=True)\n",
    "os.makedirs('config', exist_ok=True)\n",
    "os.makedirs('monitoring', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_transform as tft\n",
    "import keras_tuner as kt\n",
    "\n",
    "from absl import logging\n",
    "from keras import layers\n",
    "from tfx.components import (CsvExampleGen, Evaluator, ExampleValidator, Pusher,\n",
    "                            SchemaGen, StatisticsGen, Trainer, Transform, Tuner)\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import \\\n",
    "    LatestBlessedModelStrategy\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "from tfx.proto import example_gen_pb2, pusher_pb2, trainer_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "from typing import Text, Any, Dict, NamedTuple\n",
    "from keras_tuner.engine import base_tuner\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Modules, Config, & Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Variable Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENTS_MODULE_FILE = 'modules/heart_failure_components.py'\n",
    "PIPELINE_MODULE_FILE = 'modules/heart_failure_pipeline.py'\n",
    "TRAINER_MODULE_FILE = 'modules/heart_failure_trainer.py'\n",
    "TRANSFORM_MODULE_FILE = 'modules/heart_failure_transform.py'\n",
    "TUNER_MODULE_FILE = 'modules/heart_failure_tuner.py'\n",
    "\n",
    "CONFIG_PATH = 'config/prometheus.config'\n",
    "MONITORING_PATH = 'monitoring/prometheus.yml'\n",
    "MONITORING_DOCKER_PATH = 'monitoring/Dockerfile'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/heart_failure_components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {COMPONENTS_MODULE_FILE}\n",
    "import os\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import (CsvExampleGen, Evaluator, ExampleValidator, Pusher,\n",
    "                            SchemaGen, StatisticsGen, Trainer, Transform,\n",
    "                            Tuner)\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import \\\n",
    "    LatestBlessedModelStrategy\n",
    "from tfx.proto import example_gen_pb2, pusher_pb2, trainer_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "\n",
    "\n",
    "def init_components(args):\n",
    "    \"\"\"Initiate tfx pipeline components\n",
    "\n",
    "    Args:\n",
    "        args (dict): args that containts some dependencies\n",
    "\n",
    "    Returns:\n",
    "        tuple: TFX pipeline components\n",
    "    \"\"\"\n",
    "    output = example_gen_pb2.Output(\n",
    "        split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=8),\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=2),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    example_gen = CsvExampleGen(\n",
    "        input_base=args[\"data_dir\"],\n",
    "        output_config=output,\n",
    "    )\n",
    "\n",
    "    statistics_gen = StatisticsGen(\n",
    "        examples=example_gen.outputs[\"examples\"],\n",
    "    )\n",
    "\n",
    "    schema_gen = SchemaGen(\n",
    "        statistics=statistics_gen.outputs[\"statistics\"],\n",
    "    )\n",
    "\n",
    "    example_validator = ExampleValidator(\n",
    "        statistics=statistics_gen.outputs[\"statistics\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "    )\n",
    "\n",
    "    transform = Transform(\n",
    "        examples=example_gen.outputs[\"examples\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        module_file=os.path.abspath(args[\"transform_module\"]),\n",
    "    )\n",
    "\n",
    "    tuner = Tuner(\n",
    "        module_file=os.path.abspath(args[\"tuner_module\"]),\n",
    "        examples=transform.outputs[\"transformed_examples\"],\n",
    "        transform_graph=transform.outputs[\"transform_graph\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        train_args=trainer_pb2.TrainArgs(\n",
    "            splits=[\"train\"],\n",
    "            num_steps=args[\"train_steps\"],\n",
    "        ),\n",
    "        eval_args=trainer_pb2.EvalArgs(\n",
    "            splits=[\"eval\"],\n",
    "            num_steps=args[\"eval_steps\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        module_file=args[\"trainer_module\"],\n",
    "        examples=transform.outputs[\"transformed_examples\"],\n",
    "        transform_graph=transform.outputs[\"transform_graph\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        hyperparameters=tuner.outputs[\"best_hyperparameters\"],\n",
    "        train_args=trainer_pb2.TrainArgs(\n",
    "            splits=[\"train\"],\n",
    "            num_steps=args[\"train_steps\"],\n",
    "        ),\n",
    "        eval_args=trainer_pb2.EvalArgs(\n",
    "            splits=[\"eval\"],\n",
    "            num_steps=args[\"eval_steps\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    model_resolver = Resolver(\n",
    "        strategy_class=LatestBlessedModelStrategy,\n",
    "        model=Channel(type=Model),\n",
    "        model_blessing=Channel(type=ModelBlessing),\n",
    "    ).with_id(\"Latest_blessed_model_resolve\")\n",
    "\n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key=\"HeartDisease\")],\n",
    "        slicing_specs=[\n",
    "            tfma.SlicingSpec(),\n",
    "            tfma.SlicingSpec(feature_keys=[\"Sex\"]),\n",
    "        ],\n",
    "        metrics_specs=[\n",
    "            tfma.MetricsSpec(metrics=[\n",
    "                tfma.MetricConfig(class_name=\"AUC\"),\n",
    "                tfma.MetricConfig(class_name=\"Precision\"),\n",
    "                tfma.MetricConfig(class_name=\"Recall\"),\n",
    "                tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
    "                tfma.MetricConfig(class_name=\"TruePositives\"),\n",
    "                tfma.MetricConfig(class_name=\"FalsePositives\"),\n",
    "                tfma.MetricConfig(class_name=\"TrueNegatives\"),\n",
    "                tfma.MetricConfig(class_name=\"FalseNegatives\"),\n",
    "                tfma.MetricConfig(\n",
    "                    class_name=\"BinaryAccuracy\",\n",
    "                    threshold=tfma.MetricThreshold(\n",
    "                        value_threshold=tfma.GenericValueThreshold(\n",
    "                            lower_bound={\"value\": .6},\n",
    "                        ),\n",
    "                        change_threshold=tfma.GenericChangeThreshold(\n",
    "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                            absolute={\"value\": 1e-4},\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            ]),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    evaluator = Evaluator(\n",
    "        examples=example_gen.outputs[\"examples\"],\n",
    "        model=trainer.outputs[\"model\"],\n",
    "        baseline_model=model_resolver.outputs[\"model\"],\n",
    "        eval_config=eval_config,\n",
    "    )\n",
    "\n",
    "    pusher = Pusher(\n",
    "        model=trainer.outputs[\"model\"],\n",
    "        model_blessing=evaluator.outputs[\"blessing\"],\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                base_directory=args[\"serving_model_dir\"],\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        example_gen,\n",
    "        statistics_gen,\n",
    "        schema_gen,\n",
    "        example_validator,\n",
    "        transform,\n",
    "        tuner,\n",
    "        trainer,\n",
    "        model_resolver,\n",
    "        evaluator,\n",
    "        pusher,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/heart_failure_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {PIPELINE_MODULE_FILE}\n",
    "from typing import Text\n",
    "from absl import logging\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "\n",
    "def init_pipeline(pipeline_root: Text, pipeline_name, metadata_path, components):\n",
    "    \"\"\"Initiate tfx pipeline\n",
    "\n",
    "    Args:\n",
    "        pipeline_root (Text): a path to th pipeline directory\n",
    "        pipeline_name (str): pipeline name\n",
    "        metadata_path (str): a path to the metadata directory\n",
    "        components (dict): tfx components\n",
    "\n",
    "    Returns:\n",
    "        pipeline.Pipeline: pipeline orchestration\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(f\"Pipeline root set to: {pipeline_root}\")\n",
    "\n",
    "    beam_args = [\n",
    "        \"--direct_running_mode=multi_processing\",\n",
    "        \"----direct_num_workers=0\",\n",
    "    ]\n",
    "\n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components,\n",
    "        enable_cache=True,\n",
    "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
    "            metadata_path,\n",
    "        ),\n",
    "        eam_pipeline_args=beam_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/heart_failure_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRAINER_MODULE_FILE}\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from keras import layers\n",
    "from heart_failure_transform import (CATEGORICAL_FEATURES, LABEL_KEY, NUMERICAL_FEATURES,\n",
    "                       transformed_name)\n",
    "from heart_failure_tuner import input_fn\n",
    "\n",
    "def get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \"\"\"Return a function that parses a serialized tf.Example\"\"\"\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name=\"examples\"),\n",
    "    ])\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Return the output to be used in the serving signature.\"\"\"\n",
    "\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "\n",
    "        parsed_features = tf.io.parse_example(\n",
    "            serialized_tf_examples, feature_spec,\n",
    "        )\n",
    "\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        outputs = model(transformed_features)\n",
    "\n",
    "        return {\"outputs\": outputs}\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "def get_model(hyperparameters):\n",
    "    \"\"\"This model defines a keras Model\n",
    "\n",
    "    Args:\n",
    "        hyperparameters (kt.HyperParameters): object that contains best hyperparameters\n",
    "        from tuner\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: model as a Keras object\n",
    "    \"\"\"\n",
    "\n",
    "    input_features = []\n",
    "\n",
    "    for key, dim in CATEGORICAL_FEATURES.items():\n",
    "        input_features.append(\n",
    "            layers.Input(shape=(dim+1,), name=transformed_name(key))\n",
    "        )\n",
    "\n",
    "    for feature in NUMERICAL_FEATURES:\n",
    "        input_features.append(\n",
    "            layers.Input(shape=(1,), name=transformed_name(feature))\n",
    "        )\n",
    "\n",
    "    concatenate = layers.concatenate(input_features)\n",
    "    deep = layers.Dense(\n",
    "        hyperparameters[\"dense_unit\"], activation=tf.nn.relu)(concatenate)\n",
    "\n",
    "    for _ in range(hyperparameters[\"num_hidden_layers\"]):\n",
    "        deep = layers.Dense(\n",
    "            hyperparameters[\"dense_unit\"], activation=tf.nn.relu)(deep)\n",
    "        deep = layers.Dropout(hyperparameters[\"dropout_rate\"])(deep)\n",
    "\n",
    "    outputs = layers.Dense(1, activation=tf.nn.sigmoid)(deep)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_features, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hyperparameters[\"learning_rate\"]),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[\"binary_accuracy\"],\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_fn(fn_args):\n",
    "    \"\"\"Train the model based on given args\n",
    "\n",
    "    Args:\n",
    "        fn_args (FnArgs): Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    hyperparameters = fn_args.hyperparameters[\"values\"]\n",
    "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), \"logs\")\n",
    "\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "    train_set = input_fn(fn_args.train_files, tf_transform_output)\n",
    "    eval_set = input_fn(fn_args.eval_files, tf_transform_output)\n",
    "\n",
    "    model = get_model(hyperparameters)\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        update_freq=\"batch\"\n",
    "    )\n",
    "\n",
    "    early_stop_callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_binary_accuracy\",\n",
    "        mode=\"max\",\n",
    "        verbose=1,\n",
    "        patience=10,\n",
    "    )\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        fn_args.serving_model_dir,\n",
    "        monitor=\"val_binary_accuracy\",\n",
    "        mode=\"max\",\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        early_stop_callbacks,\n",
    "        model_checkpoint_callback\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        x=train_set,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_set,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        callbacks=callbacks,\n",
    "        epochs=hyperparameters[\"tuner/initial_epoch\"],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    signatures = {\n",
    "        \"serving_default\": get_serve_tf_examples_fn(\n",
    "            model, tf_transform_output,\n",
    "        )\n",
    "    }\n",
    "\n",
    "    model.save(\n",
    "        fn_args.serving_model_dir,\n",
    "        save_format=\"tf\",\n",
    "        signatures=signatures,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/heart_failure_transform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TRANSFORM_MODULE_FILE}\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "CATEGORICAL_FEATURES = {\n",
    "    \"Sex\": 2,\n",
    "    \"ChestPainType\": 4,\n",
    "    \"RestingECG\": 3,\n",
    "    \"ExerciseAngina\": 2,\n",
    "    \"ST_Slope\": 3,\n",
    "}\n",
    "\n",
    "NUMERICAL_FEATURES = [\n",
    "    \"Age\",\n",
    "    \"RestingBP\",\n",
    "    \"Cholesterol\",\n",
    "    \"FastingBS\",\n",
    "    \"MaxHR\",\n",
    "    \"Oldpeak\",\n",
    "]\n",
    "\n",
    "LABEL_KEY = \"HeartDisease\"\n",
    "\n",
    "\n",
    "def transformed_name(key):\n",
    "    \"\"\"Transform feature key\n",
    "\n",
    "    Args:\n",
    "        key (str): the key to be transformed\n",
    "\n",
    "    Returns:\n",
    "        str: transformed key\n",
    "    \"\"\"\n",
    "\n",
    "    return f\"{key}_xf\"\n",
    "\n",
    "\n",
    "def convert_num_to_one_hot(label_tensor, num_labels=2):\n",
    "    \"\"\"Convert a label (0 or 1) into a one-hot vector\n",
    "\n",
    "    Args:\n",
    "        label_tensor (int): label tensor (0 or 1)\n",
    "        num_labels (int, optional): num of label. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: label tensor\n",
    "    \"\"\"\n",
    "\n",
    "    one_hot_tensor = tf.one_hot(label_tensor, num_labels)\n",
    "    return tf.reshape(one_hot_tensor, [-1, num_labels])\n",
    "\n",
    "\n",
    "def replace_nan(tensor):\n",
    "    \"\"\"Replace nan value with zero number\n",
    "\n",
    "    Args:\n",
    "        tensor (list): list data with na data that want to replace\n",
    "\n",
    "    Returns:\n",
    "        list with replaced nan value\n",
    "    \"\"\"\n",
    "    tensor = tf.cast(tensor, tf.float64)\n",
    "    return tf.where(\n",
    "        tf.math.is_nan(tensor),\n",
    "        tft.mean(tensor),\n",
    "        tensor\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input features into transformed features\n",
    "\n",
    "    Args:\n",
    "        inputs (dict): map from feature keys to raw features\n",
    "\n",
    "    Returns:\n",
    "        dict: map from features keys to transformed features\n",
    "    \"\"\"\n",
    "\n",
    "    outputs = {}\n",
    "\n",
    "    for keys, values in CATEGORICAL_FEATURES.items():\n",
    "        int_value = tft.compute_and_apply_vocabulary(\n",
    "            inputs[keys], top_k=values+1)\n",
    "        outputs[transformed_name(keys)] = convert_num_to_one_hot(\n",
    "            int_value, num_labels=values+1)\n",
    "\n",
    "    for feature in NUMERICAL_FEATURES:\n",
    "        inputs[feature] = replace_nan(inputs[feature])\n",
    "        outputs[transformed_name(feature)] = tft.scale_to_0_1(inputs[feature])\n",
    "\n",
    "    outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modules/heart_failure_tuner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {TUNER_MODULE_FILE}\n",
    "from typing import Any, Dict, NamedTuple, Text\n",
    "\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from keras import layers\n",
    "from keras_tuner.engine import base_tuner\n",
    "from heart_failure_transform import (CATEGORICAL_FEATURES, LABEL_KEY, NUMERICAL_FEATURES,\n",
    "                       transformed_name)\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "TunerFnResult = NamedTuple(\"TunerFnResult\", [\n",
    "    (\"tuner\", base_tuner.BaseTuner),\n",
    "    (\"fit_kwargs\", Dict[Text, Any]),\n",
    "])\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_binary_accuracy\",\n",
    "    mode=\"max\",\n",
    "    verbose=1,\n",
    "    patience=10,\n",
    ")\n",
    "\n",
    "\n",
    "def gzip_reader_fn(filenames):\n",
    "    \"\"\"Loads compression data\n",
    "\n",
    "    Args:\n",
    "        filenames (str): a path to the data directory\n",
    "\n",
    "    Returns:\n",
    "        TfRecord: Compressed data\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
    "\n",
    "\n",
    "def input_fn(file_pattern, tf_transform_output, batch_size=64):\n",
    "    \"\"\"Generated features and labels for tuning/training\n",
    "\n",
    "    Args:\n",
    "        file_pattern: input tf_record file pattern\n",
    "        tf_transform_output: a TFTransformOutput\n",
    "        batch_size: representing the number of consecutive elements of\n",
    "        returned dataset to combine in a single batch. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        a dataset that contains (featurs, indices) tuple where features\n",
    "        is a dictionary of Tensors, and indices is a single Tensor of\n",
    "        label indices\n",
    "    \"\"\"\n",
    "\n",
    "    transform_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy()\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transform_feature_spec,\n",
    "        reader=gzip_reader_fn,\n",
    "        label_key=transformed_name(LABEL_KEY)\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_model_tuner(hyperparameters):\n",
    "    \"\"\"This function defines a keras Model\n",
    "\n",
    "    Args:\n",
    "        hyperparameters (kt.HyperParameters): object to setting hyperparameters\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: model as a Keras object\n",
    "    \"\"\"\n",
    "\n",
    "    num_hidden_layers = hyperparameters.Choice(\n",
    "        \"num_hidden_layers\",\n",
    "        values=[1, 2, 3],\n",
    "    )\n",
    "    dense_unit = hyperparameters.Int(\n",
    "        \"dense_unit\",\n",
    "        min_value=16,\n",
    "        max_value=256,\n",
    "        step=32,\n",
    "    )\n",
    "    dropout_rate = hyperparameters.Float(\n",
    "        \"dropout_rate\",\n",
    "        min_value=0.1,\n",
    "        max_value=0.9,\n",
    "        step=0.1,\n",
    "    )\n",
    "    learning_rate = hyperparameters.Choice(\n",
    "        \"learning_rate\",\n",
    "        values=[1e-2, 1e-3, 1e-4]\n",
    "    )\n",
    "\n",
    "    input_features = []\n",
    "\n",
    "    for key, dim in CATEGORICAL_FEATURES.items():\n",
    "        input_features.append(\n",
    "            layers.Input(shape=(dim+1,), name=transformed_name(key))\n",
    "        )\n",
    "\n",
    "    for feature in NUMERICAL_FEATURES:\n",
    "        input_features.append(\n",
    "            layers.Input(shape=(1,), name=transformed_name(feature))\n",
    "        )\n",
    "\n",
    "    concatenate = layers.concatenate(input_features)\n",
    "    deep = layers.Dense(dense_unit, activation=tf.nn.relu)(concatenate)\n",
    "\n",
    "    for _ in range(num_hidden_layers):\n",
    "        deep = layers.Dense(dense_unit, activation=tf.nn.relu)(deep)\n",
    "        deep = layers.Dropout(dropout_rate)(deep)\n",
    "\n",
    "    outputs = layers.Dense(1, activation=tf.nn.sigmoid)(deep)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_features, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[\"binary_accuracy\"],\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def tuner_fn(fn_args):\n",
    "    \"\"\"Tuning the model to get the best hyperparameters based on given args\n",
    "\n",
    "    Args:\n",
    "        fn_args (FnArgs): Holds args used to train the model as name/value pair\n",
    "\n",
    "    Returns:\n",
    "        TunerFnResult (NamedTuple): object to run model tuner\n",
    "    \"\"\"\n",
    "\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    train_set = input_fn(fn_args.train_files[0], tf_transform_output,)\n",
    "    eval_set = input_fn(fn_args.eval_files[0], tf_transform_output,)\n",
    "\n",
    "    tuner = kt.Hyperband(\n",
    "        hypermodel=get_model_tuner,\n",
    "        objective=kt.Objective(\n",
    "            \"val_loss\",\n",
    "            direction=\"min\",\n",
    "        ),\n",
    "        max_epochs=NUM_EPOCHS,\n",
    "        factor=3,\n",
    "        directory=fn_args.working_dir,\n",
    "        project_name=\"kt_hyperband\",\n",
    "    )\n",
    "\n",
    "    return TunerFnResult(\n",
    "        tuner=tuner,\n",
    "        fit_kwargs={\n",
    "            \"x\": train_set,\n",
    "            \"validation_data\": eval_set,\n",
    "            \"steps_per_epoch\": fn_args.train_steps,\n",
    "            \"validation_steps\": fn_args.eval_steps,\n",
    "            \"callbacks\": [early_stop]\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config/prometheus.config\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CONFIG_PATH}\n",
    "prometheus_config {\n",
    "   enable: true,\n",
    "   path: \"/monitoring/prometheus/metrics\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Monitoring Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing monitoring/prometheus.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MONITORING_PATH}\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "  evaluation_interval: 15s\n",
    "  external_labels:\n",
    "    monitor: \"tf-serving-monitor\"\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: \"prometheus\"\n",
    "    scrape_interval: 5s\n",
    "    metrics_path: /monitoring/prometheus/metrics\n",
    "    static_configs:\n",
    "      - targets: [\"hf-pred.herokuapp.com\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Monitoring Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing monitoring/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MONITORING_DOCKER_PATH}\n",
    "FROM prom/prometheus:latest\n",
    "\n",
    "COPY prometheus.yml /etc/prometheus/prometheus.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPainType</th>\n",
       "      <th>RestingBP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>FastingBS</th>\n",
       "      <th>RestingECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExerciseAngina</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>ST_Slope</th>\n",
       "      <th>HeartDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>ATA</td>\n",
       "      <td>140</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Up</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  MaxHR  \\\n",
       "0   40   M           ATA        140          289          0     Normal    172   \n",
       "\n",
       "  ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n",
       "0              N      0.0       Up             0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_dataset = 'data/heart.csv'\n",
    "df = pd.read_csv(url_dataset)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Age             918 non-null    int64  \n",
      " 1   Sex             918 non-null    object \n",
      " 2   ChestPainType   918 non-null    object \n",
      " 3   RestingBP       918 non-null    int64  \n",
      " 4   Cholesterol     918 non-null    int64  \n",
      " 5   FastingBS       918 non-null    int64  \n",
      " 6   RestingECG      918 non-null    object \n",
      " 7   MaxHR           918 non-null    int64  \n",
      " 8   ExerciseAngina  918 non-null    object \n",
      " 9   Oldpeak         918 non-null    float64\n",
      " 10  ST_Slope        918 non-null    object \n",
      " 11  HeartDisease    918 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(5)\n",
      "memory usage: 86.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah nilai 0 pada RestingBP: 1\n",
      "Jumlah nilai 0 pada Cholesterol: 172\n"
     ]
    }
   ],
   "source": [
    "RestingBP = (df.RestingBP == 0).sum()\n",
    "Cholesterol = (df.Cholesterol == 0).sum()\n",
    "\n",
    "print(f'Jumlah nilai 0 pada RestingBP: {RestingBP}')\n",
    "print(f'Jumlah nilai 0 pada Cholesterol: {Cholesterol}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(746, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df.loc[(df['RestingBP']==0)].index, inplace=True)\n",
    "df.drop(df.loc[(df['Cholesterol']==0)].index, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data_cleaned'\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "df.to_csv(os.path.join(data_path, 'heart_failure_cleaned.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import heart_failure_components, heart_failure_pipeline\n",
    "\n",
    "PIPELINE_NAME = 'aflahazzaky-pipeline'\n",
    "\n",
    "# Pipeline inputs\n",
    "DATA_ROOT = 'data_cleaned'\n",
    "TRAINER_MODULE_FILE = 'modules/heart_failure_trainer.py'\n",
    "TRANSFORM_MODULE_FILE = 'modules/heart_failure_transform.py'\n",
    "TUNER_MODULE_FILE = 'modules/heart_failure_tuner.py'\n",
    "\n",
    "# Pipeline outputs\n",
    "OUTPUT_ROOT = 'outputs'\n",
    "\n",
    "serving_model_dir = os.path.join(OUTPUT_ROOT, 'serving_model')\n",
    "pipeline_root = os.path.join(OUTPUT_ROOT, PIPELINE_NAME)\n",
    "metadata_path = os.path.join(pipeline_root, 'metadata.sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 15s]\n",
      "val_loss: 1.0778942108154297\n",
      "\n",
      "Best val_loss So Far: 0.34886494278907776\n",
      "Total elapsed time: 00h 01m 54s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in outputs\\aflahazzaky-pipeline\\Tuner\\.system\\executor_execution\\7\\.temp\\7\\kt_hyperband\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x00000164A685F5B0>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 3\n",
      "dense_unit: 112\n",
      "dropout_rate: 0.8\n",
      "learning_rate: 0.0001\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0003\n",
      "Score: 0.34886494278907776\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 1\n",
      "dense_unit: 16\n",
      "dropout_rate: 0.7000000000000001\n",
      "learning_rate: 0.0001\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0000\n",
      "Score: 0.38231703639030457\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 3\n",
      "dense_unit: 112\n",
      "dropout_rate: 0.8\n",
      "learning_rate: 0.0001\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.3930197060108185\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 1\n",
      "dense_unit: 16\n",
      "dropout_rate: 0.7000000000000001\n",
      "learning_rate: 0.0001\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.3963373899459839\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 1\n",
      "dense_unit: 48\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.42022696137428284\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 2\n",
      "dense_unit: 176\n",
      "dropout_rate: 0.7000000000000001\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.9629871249198914\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 3\n",
      "dense_unit: 144\n",
      "dropout_rate: 0.5\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.9979807138442993\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 3\n",
      "dense_unit: 80\n",
      "dropout_rate: 0.7000000000000001\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 1.0778942108154297\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 3\n",
      "dense_unit: 176\n",
      "dropout_rate: 0.5\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 1.1707525253295898\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_hidden_layers: 2\n",
      "dense_unit: 144\n",
      "dropout_rate: 0.2\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 5\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 1.5009188652038574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Sex_xf (InputLayer)            [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " ChestPainType_xf (InputLayer)  [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " RestingECG_xf (InputLayer)     [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " ExerciseAngina_xf (InputLayer)  [(None, 3)]         0           []                               \n",
      "                                                                                                  \n",
      " ST_Slope_xf (InputLayer)       [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " Age_xf (InputLayer)            [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " RestingBP_xf (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Cholesterol_xf (InputLayer)    [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " FastingBS_xf (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " MaxHR_xf (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Oldpeak_xf (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 25)           0           ['Sex_xf[0][0]',                 \n",
      "                                                                  'ChestPainType_xf[0][0]',       \n",
      "                                                                  'RestingECG_xf[0][0]',          \n",
      "                                                                  'ExerciseAngina_xf[0][0]',      \n",
      "                                                                  'ST_Slope_xf[0][0]',            \n",
      "                                                                  'Age_xf[0][0]',                 \n",
      "                                                                  'RestingBP_xf[0][0]',           \n",
      "                                                                  'Cholesterol_xf[0][0]',         \n",
      "                                                                  'FastingBS_xf[0][0]',           \n",
      "                                                                  'MaxHR_xf[0][0]',               \n",
      "                                                                  'Oldpeak_xf[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 112)          2912        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 112)          12656       ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 112)          0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 112)          12656       ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 112)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 112)          12656       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 112)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 1)            113         ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 40,993\n",
      "Trainable params: 40,993\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2\n",
      " 980/1000 [============================>.] - ETA: 0s - loss: 0.6988 - binary_accuracy: 0.5620\n",
      "Epoch 1: val_binary_accuracy improved from -inf to 0.84023, saving model to outputs\\aflahazzaky-pipeline\\Trainer\\model\\8\\Format-Serving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Sex_xf, ChestPainType_xf, RestingECG_xf, ExerciseAngina_xf, ST_Slope_xf, Age_xf, RestingBP_xf, Cholesterol_xf, FastingBS_xf, MaxHR_xf, Oldpeak_xf with unsupported characters which will be renamed to sex_xf, chestpaintype_xf, restingecg_xf, exerciseangina_xf, st_slope_xf, age_xf, restingbp_xf, cholesterol_xf, fastingbs_xf, maxhr_xf, oldpeak_xf in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs\\aflahazzaky-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs\\aflahazzaky-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 5s 4ms/step - loss: 0.6971 - binary_accuracy: 0.5644 - val_loss: 0.5813 - val_binary_accuracy: 0.8402\n",
      "Epoch 2/2\n",
      " 986/1000 [============================>.] - ETA: 0s - loss: 0.5043 - binary_accuracy: 0.7666\n",
      "Epoch 2: val_binary_accuracy did not improve from 0.84023\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 0.5035 - binary_accuracy: 0.7674 - val_loss: 0.3780 - val_binary_accuracy: 0.8402\n",
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs\\aflahazzaky-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: outputs\\aflahazzaky-pipeline\\Trainer\\model\\8\\Format-Serving\\assets\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000164A5337340> and <keras.engine.input_layer.InputLayer object at 0x00000164A5431130>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000164A5337340> and <keras.engine.input_layer.InputLayer object at 0x00000164A5431130>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000164C16BDCA0> and <keras.engine.input_layer.InputLayer object at 0x000001649EAF27F0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000164C16BDCA0> and <keras.engine.input_layer.InputLayer object at 0x000001649EAF27F0>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000164A6ACF3A0> and <keras.engine.input_layer.InputLayer object at 0x000001649E8C17C0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000164A6ACF3A0> and <keras.engine.input_layer.InputLayer object at 0x000001649E8C17C0>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000164B6B22850> and <keras.engine.input_layer.InputLayer object at 0x00000164A6ADD520>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000164B6B22850> and <keras.engine.input_layer.InputLayer object at 0x00000164A6ADD520>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001649FBD9D60> and <keras.engine.input_layer.InputLayer object at 0x00000164C1748820>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001649FBD9D60> and <keras.engine.input_layer.InputLayer object at 0x00000164C1748820>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000164C1680400> and <keras.engine.input_layer.InputLayer object at 0x00000164A66232E0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000164C1680400> and <keras.engine.input_layer.InputLayer object at 0x00000164A66232E0>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000164C12A1940> and <keras.engine.input_layer.InputLayer object at 0x00000164C1352A60>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000164C12A1940> and <keras.engine.input_layer.InputLayer object at 0x00000164C1352A60>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000164C7892B20> and <keras.engine.input_layer.InputLayer object at 0x00000164C132BD90>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000164C7892B20> and <keras.engine.input_layer.InputLayer object at 0x00000164C132BD90>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From h:\\Submission Dicoding\\Submission_MLOPS_Dicoding_Aflah_2\\.venv\\lib\\site-packages\\tensorflow_model_analysis\\writers\\metrics_plots_and_validations_writer.py:110: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From h:\\Submission Dicoding\\Submission_MLOPS_Dicoding_Aflah_2\\.venv\\lib\\site-packages\\tensorflow_model_analysis\\writers\\metrics_plots_and_validations_writer.py:110: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n"
     ]
    }
   ],
   "source": [
    "components_args = {\n",
    "    \"data_dir\": DATA_ROOT,\n",
    "    \"trainer_module\": TRAINER_MODULE_FILE,\n",
    "    \"tuner_module\": TUNER_MODULE_FILE,\n",
    "    \"transform_module\": TRANSFORM_MODULE_FILE,\n",
    "    \"train_steps\": 1000,\n",
    "    \"eval_steps\": 800,\n",
    "    \"serving_model_dir\": serving_model_dir,\n",
    "}\n",
    "\n",
    "components = heart_failure_components.init_components(components_args)\n",
    "\n",
    "pipeline = heart_failure_pipeline.init_pipeline(\n",
    "    pipeline_root, \n",
    "    PIPELINE_NAME, \n",
    "    metadata_path, \n",
    "    components\n",
    ")\n",
    "BeamDagRunner().run(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
