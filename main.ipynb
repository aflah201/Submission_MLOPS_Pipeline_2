{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyek Pengembangan dan Pengoperasian Sistem Machine Learning**\n",
    "\n",
    "* Nama : Moh. Aflah Azzaky\n",
    "* Email : aflahazzaki123@gmail.com\n",
    "* ID Dicoding : aflahazzaky\n",
    "* Dataset : [Heart Failure Prediction Dataset](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Buat folder 'modules, config, monitoring' jika belum ada\n",
    "os.makedirs('modules', exist_ok=True)\n",
    "os.makedirs('config', exist_ok=True)\n",
    "os.makedirs('monitoring', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_transform as tft\n",
    "import keras_tuner as kt\n",
    "\n",
    "from absl import logging\n",
    "from keras import layers\n",
    "from tfx.components import (CsvExampleGen, Evaluator, ExampleValidator, Pusher,\n",
    "                            SchemaGen, StatisticsGen, Trainer, Transform, Tuner)\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import \\\n",
    "    LatestBlessedModelStrategy\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "from tfx.proto import example_gen_pb2, pusher_pb2, trainer_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "from typing import Text, Any, Dict, NamedTuple\n",
    "from keras_tuner.engine import base_tuner\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Modules, Config, & Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Variable Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENTS_MODULE_FILE = 'modules/heart_failure_components.py'\n",
    "PIPELINE_MODULE_FILE = 'modules/heart_failure_pipeline.py'\n",
    "TRAINER_MODULE_FILE = 'modules/heart_failure_trainer.py'\n",
    "TRANSFORM_MODULE_FILE = 'modules/heart_failure_transform.py'\n",
    "TUNER_MODULE_FILE = 'modules/heart_failure_tuner.py'\n",
    "\n",
    "CONFIG_PATH = 'config/prometheus.config'\n",
    "MONITORING_PATH = 'monitoring/prometheus.yml'\n",
    "MONITORING_DOCKER_PATH = 'monitoring/Dockerfile'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {COMPONENTS_MODULE_FILE}\n",
    "import os\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import (CsvExampleGen, Evaluator, ExampleValidator, Pusher,\n",
    "                            SchemaGen, StatisticsGen, Trainer, Transform,\n",
    "                            Tuner)\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import \\\n",
    "    LatestBlessedModelStrategy\n",
    "from tfx.proto import example_gen_pb2, pusher_pb2, trainer_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "\n",
    "\n",
    "def init_components(args):\n",
    "    \"\"\"Initiate tfx pipeline components\n",
    "\n",
    "    Args:\n",
    "        args (dict): args that containts some dependencies\n",
    "\n",
    "    Returns:\n",
    "        tuple: TFX pipeline components\n",
    "    \"\"\"\n",
    "    output = example_gen_pb2.Output(\n",
    "        split_config=example_gen_pb2.SplitConfig(splits=[\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=8),\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=2),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    example_gen = CsvExampleGen(\n",
    "        input_base=args[\"data_dir\"],\n",
    "        output_config=output,\n",
    "    )\n",
    "\n",
    "    statistics_gen = StatisticsGen(\n",
    "        examples=example_gen.outputs[\"examples\"],\n",
    "    )\n",
    "\n",
    "    schema_gen = SchemaGen(\n",
    "        statistics=statistics_gen.outputs[\"statistics\"],\n",
    "    )\n",
    "\n",
    "    example_validator = ExampleValidator(\n",
    "        statistics=statistics_gen.outputs[\"statistics\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "    )\n",
    "\n",
    "    transform = Transform(\n",
    "        examples=example_gen.outputs[\"examples\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        module_file=os.path.abspath(args[\"transform_module\"]),\n",
    "    )\n",
    "\n",
    "    tuner = Tuner(\n",
    "        module_file=os.path.abspath(args[\"tuner_module\"]),\n",
    "        examples=transform.outputs[\"transformed_examples\"],\n",
    "        transform_graph=transform.outputs[\"transform_graph\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        train_args=trainer_pb2.TrainArgs(\n",
    "            splits=[\"train\"],\n",
    "            num_steps=args[\"train_steps\"],\n",
    "        ),\n",
    "        eval_args=trainer_pb2.EvalArgs(\n",
    "            splits=[\"eval\"],\n",
    "            num_steps=args[\"eval_steps\"],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        module_file=args[\"trainer_module\"],\n",
    "        examples=transform.outputs[\"transformed_examples\"],\n",
    "        transform_graph=transform.outputs[\"transform_graph\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        hyperparameters=tuner.outputs[\"best_hyperparameters\"],\n",
    "        train_args=trainer_pb2.TrainArgs(\n",
    "            splits=[\"train\"],\n",
    "            num_steps=args[\"train_steps\"],\n",
    "        ),\n",
    "        eval_args=trainer_pb2.EvalArgs(\n",
    "            splits=[\"eval\"],\n",
    "            num_steps=args[\"eval_steps\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    model_resolver = Resolver(\n",
    "        strategy_class=LatestBlessedModelStrategy,\n",
    "        model=Channel(type=Model),\n",
    "        model_blessing=Channel(type=ModelBlessing),\n",
    "    ).with_id(\"Latest_blessed_model_resolve\")\n",
    "\n",
    "    eval_config = tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key=\"HeartDisease\")],\n",
    "        slicing_specs=[\n",
    "            tfma.SlicingSpec(),\n",
    "            tfma.SlicingSpec(feature_keys=[\"Sex\"]),\n",
    "        ],\n",
    "        metrics_specs=[\n",
    "            tfma.MetricsSpec(metrics=[\n",
    "                tfma.MetricConfig(class_name=\"AUC\"),\n",
    "                tfma.MetricConfig(class_name=\"Precision\"),\n",
    "                tfma.MetricConfig(class_name=\"Recall\"),\n",
    "                tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
    "                tfma.MetricConfig(class_name=\"TruePositives\"),\n",
    "                tfma.MetricConfig(class_name=\"FalsePositives\"),\n",
    "                tfma.MetricConfig(class_name=\"TrueNegatives\"),\n",
    "                tfma.MetricConfig(class_name=\"FalseNegatives\"),\n",
    "                tfma.MetricConfig(\n",
    "                    class_name=\"BinaryAccuracy\",\n",
    "                    threshold=tfma.MetricThreshold(\n",
    "                        value_threshold=tfma.GenericValueThreshold(\n",
    "                            lower_bound={\"value\": .6},\n",
    "                        ),\n",
    "                        change_threshold=tfma.GenericChangeThreshold(\n",
    "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                            absolute={\"value\": 1e-4},\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            ]),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    evaluator = Evaluator(\n",
    "        examples=example_gen.outputs[\"examples\"],\n",
    "        model=trainer.outputs[\"model\"],\n",
    "        baseline_model=model_resolver.outputs[\"model\"],\n",
    "        eval_config=eval_config,\n",
    "    )\n",
    "\n",
    "    pusher = Pusher(\n",
    "        model=trainer.outputs[\"model\"],\n",
    "        model_blessing=evaluator.outputs[\"blessing\"],\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "                base_directory=args[\"serving_model_dir\"],\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        example_gen,\n",
    "        statistics_gen,\n",
    "        schema_gen,\n",
    "        example_validator,\n",
    "        transform,\n",
    "        tuner,\n",
    "        trainer,\n",
    "        model_resolver,\n",
    "        evaluator,\n",
    "        pusher,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {PIPELINE_MODULE_FILE}\n",
    "from typing import Text\n",
    "from absl import logging\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "\n",
    "def init_pipeline(pipeline_root: Text, pipeline_name, metadata_path, components):\n",
    "    \"\"\"Initiate tfx pipeline\n",
    "\n",
    "    Args:\n",
    "        pipeline_root (Text): a path to th pipeline directory\n",
    "        pipeline_name (str): pipeline name\n",
    "        metadata_path (str): a path to the metadata directory\n",
    "        components (dict): tfx components\n",
    "\n",
    "    Returns:\n",
    "        pipeline.Pipeline: pipeline orchestration\n",
    "    \"\"\"\n",
    "\n",
    "    logging.info(f\"Pipeline root set to: {pipeline_root}\")\n",
    "\n",
    "    beam_args = [\n",
    "        \"--direct_running_mode=multi_processing\",\n",
    "        \"----direct_num_workers=0\",\n",
    "    ]\n",
    "\n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components,\n",
    "        enable_cache=True,\n",
    "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
    "            metadata_path,\n",
    "        ),\n",
    "        eam_pipeline_args=beam_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRAINER_MODULE_FILE}\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from keras import layers\n",
    "from heart_failure_transform import (CATEGORICAL_FEATURES, LABEL_KEY, NUMERICAL_FEATURES,\n",
    "                       transformed_name)\n",
    "from heart_failure_tuner import input_fn\n",
    "\n",
    "def get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \"\"\"Return a function that parses a serialized tf.Example\"\"\"\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name=\"examples\"),\n",
    "    ])\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Return the output to be used in the serving signature.\"\"\"\n",
    "\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "\n",
    "        parsed_features = tf.io.parse_example(\n",
    "            serialized_tf_examples, feature_spec,\n",
    "        )\n",
    "\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        outputs = model(transformed_features)\n",
    "\n",
    "        return {\"outputs\": outputs}\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "def get_model(hyperparameters):\n",
    "    \"\"\"This model defines a keras Model\n",
    "\n",
    "    Args:\n",
    "        hyperparameters (kt.HyperParameters): object that contains best hyperparameters\n",
    "        from tuner\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: model as a Keras object\n",
    "    \"\"\"\n",
    "\n",
    "    input_features = []\n",
    "\n",
    "    for key, dim in CATEGORICAL_FEATURES.items():\n",
    "        input_features.append(\n",
    "            layers.Input(shape=(dim+1,), name=transformed_name(key))\n",
    "        )\n",
    "\n",
    "    for feature in NUMERICAL_FEATURES:\n",
    "        input_features.append(\n",
    "            layers.Input(shape=(1,), name=transformed_name(feature))\n",
    "        )\n",
    "\n",
    "    concatenate = layers.concatenate(input_features)\n",
    "    deep = layers.Dense(\n",
    "        hyperparameters[\"dense_unit\"], activation=tf.nn.relu)(concatenate)\n",
    "\n",
    "    for _ in range(hyperparameters[\"num_hidden_layers\"]):\n",
    "        deep = layers.Dense(\n",
    "            hyperparameters[\"dense_unit\"], activation=tf.nn.relu)(deep)\n",
    "        deep = layers.Dropout(hyperparameters[\"dropout_rate\"])(deep)\n",
    "\n",
    "    outputs = layers.Dense(1, activation=tf.nn.sigmoid)(deep)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_features, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hyperparameters[\"learning_rate\"]),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[\"binary_accuracy\"],\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_fn(fn_args):\n",
    "    \"\"\"Train the model based on given args\n",
    "\n",
    "    Args:\n",
    "        fn_args (FnArgs): Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    hyperparameters = fn_args.hyperparameters[\"values\"]\n",
    "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), \"logs\")\n",
    "\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "    train_set = input_fn(fn_args.train_files, tf_transform_output)\n",
    "    eval_set = input_fn(fn_args.eval_files, tf_transform_output)\n",
    "\n",
    "    model = get_model(hyperparameters)\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        update_freq=\"batch\"\n",
    "    )\n",
    "\n",
    "    early_stop_callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_binary_accuracy\",\n",
    "        mode=\"max\",\n",
    "        verbose=1,\n",
    "        patience=10,\n",
    "    )\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        fn_args.serving_model_dir,\n",
    "        monitor=\"val_binary_accuracy\",\n",
    "        mode=\"max\",\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        tensorboard_callback,\n",
    "        early_stop_callbacks,\n",
    "        model_checkpoint_callback\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        x=train_set,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_set,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        callbacks=callbacks,\n",
    "        epochs=hyperparameters[\"tuner/initial_epoch\"],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    signatures = {\n",
    "        \"serving_default\": get_serve_tf_examples_fn(\n",
    "            model, tf_transform_output,\n",
    "        )\n",
    "    }\n",
    "\n",
    "    model.save(\n",
    "        fn_args.serving_model_dir,\n",
    "        save_format=\"tf\",\n",
    "        signatures=signatures,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TRANSFORM_MODULE_FILE}\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "CATEGORICAL_FEATURES = {\n",
    "    \"Sex\": 2,\n",
    "    \"ChestPainType\": 4,\n",
    "    \"RestingECG\": 3,\n",
    "    \"ExerciseAngina\": 2,\n",
    "    \"ST_Slope\": 3,\n",
    "}\n",
    "\n",
    "NUMERICAL_FEATURES = [\n",
    "    \"Age\",\n",
    "    \"RestingBP\",\n",
    "    \"Cholesterol\",\n",
    "    \"FastingBS\",\n",
    "    \"MaxHR\",\n",
    "    \"Oldpeak\",\n",
    "]\n",
    "\n",
    "LABEL_KEY = \"HeartDisease\"\n",
    "\n",
    "\n",
    "def transformed_name(key):\n",
    "    \"\"\"Transform feature key\n",
    "\n",
    "    Args:\n",
    "        key (str): the key to be transformed\n",
    "\n",
    "    Returns:\n",
    "        str: transformed key\n",
    "    \"\"\"\n",
    "\n",
    "    return f\"{key}_xf\"\n",
    "\n",
    "\n",
    "def convert_num_to_one_hot(label_tensor, num_labels=2):\n",
    "    \"\"\"Convert a label (0 or 1) into a one-hot vector\n",
    "\n",
    "    Args:\n",
    "        label_tensor (int): label tensor (0 or 1)\n",
    "        num_labels (int, optional): num of label. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: label tensor\n",
    "    \"\"\"\n",
    "\n",
    "    one_hot_tensor = tf.one_hot(label_tensor, num_labels)\n",
    "    return tf.reshape(one_hot_tensor, [-1, num_labels])\n",
    "\n",
    "\n",
    "def replace_nan(tensor):\n",
    "    \"\"\"Replace nan value with zero number\n",
    "\n",
    "    Args:\n",
    "        tensor (list): list data with na data that want to replace\n",
    "\n",
    "    Returns:\n",
    "        list with replaced nan value\n",
    "    \"\"\"\n",
    "    tensor = tf.cast(tensor, tf.float64)\n",
    "    return tf.where(\n",
    "        tf.math.is_nan(tensor),\n",
    "        tft.mean(tensor),\n",
    "        tensor\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input features into transformed features\n",
    "\n",
    "    Args:\n",
    "        inputs (dict): map from feature keys to raw features\n",
    "\n",
    "    Returns:\n",
    "        dict: map from features keys to transformed features\n",
    "    \"\"\"\n",
    "\n",
    "    outputs = {}\n",
    "\n",
    "    for keys, values in CATEGORICAL_FEATURES.items():\n",
    "        int_value = tft.compute_and_apply_vocabulary(\n",
    "            inputs[keys], top_k=values+1)\n",
    "        outputs[transformed_name(keys)] = convert_num_to_one_hot(\n",
    "            int_value, num_labels=values+1)\n",
    "\n",
    "    for feature in NUMERICAL_FEATURES:\n",
    "        inputs[feature] = replace_nan(inputs[feature])\n",
    "        outputs[transformed_name(feature)] = tft.scale_to_0_1(inputs[feature])\n",
    "\n",
    "    outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {TUNER_MODULE_FILE}\n",
    "from typing import Any, Dict, NamedTuple, Text\n",
    "\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from keras import layers\n",
    "from keras_tuner.engine import base_tuner\n",
    "from heart_failure_transform import (CATEGORICAL_FEATURES, LABEL_KEY, NUMERICAL_FEATURES,\n",
    "                       transformed_name)\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "TunerFnResult = NamedTuple(\"TunerFnResult\", [\n",
    "    (\"tuner\", base_tuner.BaseTuner),\n",
    "    (\"fit_kwargs\", Dict[Text, Any]),\n",
    "])\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_binary_accuracy\",\n",
    "    mode=\"max\",\n",
    "    verbose=1,\n",
    "    patience=10,\n",
    ")\n",
    "\n",
    "\n",
    "def gzip_reader_fn(filenames):\n",
    "    \"\"\"Loads compression data\n",
    "\n",
    "    Args:\n",
    "        filenames (str): a path to the data directory\n",
    "\n",
    "    Returns:\n",
    "        TfRecord: Compressed data\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
    "\n",
    "\n",
    "def input_fn(file_pattern, tf_transform_output, batch_size=64):\n",
    "    \"\"\"Generated features and labels for tuning/training\n",
    "\n",
    "    Args:\n",
    "        file_pattern: input tf_record file pattern\n",
    "        tf_transform_output: a TFTransformOutput\n",
    "        batch_size: representing the number of consecutive elements of\n",
    "        returned dataset to combine in a single batch. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        a dataset that contains (featurs, indices) tuple where features\n",
    "        is a dictionary of Tensors, and indices is a single Tensor of\n",
    "        label indices\n",
    "    \"\"\"\n",
    "\n",
    "    transform_feature_spec = (\n",
    "        tf_transform_output.transformed_feature_spec().copy()\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=transform_feature_spec,\n",
    "        reader=gzip_reader_fn,\n",
    "        label_key=transformed_name(LABEL_KEY)\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_model_tuner(hyperparameters):\n",
    "    \"\"\"This function defines a keras Model\n",
    "\n",
    "    Args:\n",
    "        hyperparameters (kt.HyperParameters): object to setting hyperparameters\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: model as a Keras object\n",
    "    \"\"\"\n",
    "\n",
    "    num_hidden_layers = hyperparameters.Choice(\n",
    "        \"num_hidden_layers\",\n",
    "        values=[1, 2, 3],\n",
    "    )\n",
    "    dense_unit = hyperparameters.Int(\n",
    "        \"dense_unit\",\n",
    "        min_value=16,\n",
    "        max_value=256,\n",
    "        step=32,\n",
    "    )\n",
    "    dropout_rate = hyperparameters.Float(\n",
    "        \"dropout_rate\",\n",
    "        min_value=0.1,\n",
    "        max_value=0.9,\n",
    "        step=0.1,\n",
    "    )\n",
    "    learning_rate = hyperparameters.Choice(\n",
    "        \"learning_rate\",\n",
    "        values=[1e-2, 1e-3, 1e-4]\n",
    "    )\n",
    "\n",
    "    input_features = []\n",
    "\n",
    "    for key, dim in CATEGORICAL_FEATURES.items():\n",
    "        input_features.append(\n",
    "            layers.Input(shape=(dim+1,), name=transformed_name(key))\n",
    "        )\n",
    "\n",
    "    for feature in NUMERICAL_FEATURES:\n",
    "        input_features.append(\n",
    "            layers.Input(shape=(1,), name=transformed_name(feature))\n",
    "        )\n",
    "\n",
    "    concatenate = layers.concatenate(input_features)\n",
    "    deep = layers.Dense(dense_unit, activation=tf.nn.relu)(concatenate)\n",
    "\n",
    "    for _ in range(num_hidden_layers):\n",
    "        deep = layers.Dense(dense_unit, activation=tf.nn.relu)(deep)\n",
    "        deep = layers.Dropout(dropout_rate)(deep)\n",
    "\n",
    "    outputs = layers.Dense(1, activation=tf.nn.sigmoid)(deep)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_features, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[\"binary_accuracy\"],\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def tuner_fn(fn_args):\n",
    "    \"\"\"Tuning the model to get the best hyperparameters based on given args\n",
    "\n",
    "    Args:\n",
    "        fn_args (FnArgs): Holds args used to train the model as name/value pair\n",
    "\n",
    "    Returns:\n",
    "        TunerFnResult (NamedTuple): object to run model tuner\n",
    "    \"\"\"\n",
    "\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
    "\n",
    "    train_set = input_fn(fn_args.train_files[0], tf_transform_output,)\n",
    "    eval_set = input_fn(fn_args.eval_files[0], tf_transform_output,)\n",
    "\n",
    "    tuner = kt.Hyperband(\n",
    "        hypermodel=get_model_tuner,\n",
    "        objective=kt.Objective(\n",
    "            \"val_loss\",\n",
    "            direction=\"min\",\n",
    "        ),\n",
    "        max_epochs=NUM_EPOCHS,\n",
    "        factor=3,\n",
    "        directory=fn_args.working_dir,\n",
    "        project_name=\"kt_hyperband\",\n",
    "    )\n",
    "\n",
    "    return TunerFnResult(\n",
    "        tuner=tuner,\n",
    "        fit_kwargs={\n",
    "            \"x\": train_set,\n",
    "            \"validation_data\": eval_set,\n",
    "            \"steps_per_epoch\": fn_args.train_steps,\n",
    "            \"validation_steps\": fn_args.eval_steps,\n",
    "            \"callbacks\": [early_stop]\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {CONFIG_PATH}\n",
    "prometheus_config {\n",
    "   enable: true,\n",
    "   path: \"/monitoring/prometheus/metrics\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Monitoring Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {MONITORING_PATH}\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "  evaluation_interval: 15s\n",
    "  external_labels:\n",
    "    monitor: \"tf-serving-monitor\"\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: \"prometheus\"\n",
    "    scrape_interval: 5s\n",
    "    metrics_path: /monitoring/prometheus/metrics\n",
    "    static_configs:\n",
    "      - targets: [\"hf-pred.herokuapp.com\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Monitoring Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {MONITORING_DOCKER_PATH}\n",
    "FROM prom/prometheus:latest\n",
    "\n",
    "COPY prometheus.yml /etc/prometheus/prometheus.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dataset = 'data/heart.csv'\n",
    "df = pd.read_csv(url_dataset)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RestingBP = (df.RestingBP == 0).sum()\n",
    "Cholesterol = (df.Cholesterol == 0).sum()\n",
    "\n",
    "print(f'Jumlah nilai 0 pada RestingBP: {RestingBP}')\n",
    "print(f'Jumlah nilai 0 pada Cholesterol: {Cholesterol}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.loc[(df['RestingBP']==0)].index, inplace=True)\n",
    "df.drop(df.loc[(df['Cholesterol']==0)].index, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data_cleaned'\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "df.to_csv(os.path.join(data_path, 'heart_failure_cleaned.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import heart_failure_components, heart_failure_pipeline\n",
    "\n",
    "PIPELINE_NAME = 'aflahazzaky-pipeline'\n",
    "\n",
    "# Pipeline inputs\n",
    "DATA_ROOT = 'data_cleaned'\n",
    "TRAINER_MODULE_FILE = 'modules/heart_failure_trainer.py'\n",
    "TRANSFORM_MODULE_FILE = 'modules/heart_failure_transform.py'\n",
    "TUNER_MODULE_FILE = 'modules/heart_failure_tuner.py'\n",
    "\n",
    "# Pipeline outputs\n",
    "OUTPUT_ROOT = 'outputs'\n",
    "\n",
    "serving_model_dir = os.path.join(OUTPUT_ROOT, 'serving_model')\n",
    "pipeline_root = os.path.join(OUTPUT_ROOT, PIPELINE_NAME)\n",
    "metadata_path = os.path.join(pipeline_root, 'metadata.sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_args = {\n",
    "    \"data_dir\": DATA_ROOT,\n",
    "    \"trainer_module\": TRAINER_MODULE_FILE,\n",
    "    \"tuner_module\": TUNER_MODULE_FILE,\n",
    "    \"transform_module\": TRANSFORM_MODULE_FILE,\n",
    "    \"train_steps\": 1000,\n",
    "    \"eval_steps\": 800,\n",
    "    \"serving_model_dir\": serving_model_dir,\n",
    "}\n",
    "\n",
    "components = heart_failure_components.init_components(components_args)\n",
    "\n",
    "pipeline = heart_failure_pipeline.init_pipeline(\n",
    "    pipeline_root, \n",
    "    PIPELINE_NAME, \n",
    "    metadata_path, \n",
    "    components\n",
    ")\n",
    "BeamDagRunner().run(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
